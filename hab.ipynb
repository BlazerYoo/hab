{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "habV2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3yR9wa1bPeN"
      },
      "source": [
        "!pip install -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgngrcC5xCaK"
      },
      "source": [
        "# Load prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k3ZxvEYbQc3"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMDgrU00bXYk"
      },
      "source": [
        "# Import packages\n",
        "import os, numpy as np, matplotlib.pyplot as plt, tensorflow as tf, tensorflow_datasets as tfds, kerastuner as kt\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "print('Num GPUs Available: ', len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGwiQk52xCaL"
      },
      "source": [
        "# Locate custom dataset directory\n",
        "%cd /content/gdrive/My Drive/Colab/plankton/data/tfds_dataset\n",
        "# Create custom dataset\n",
        "#!tfds new plankton_images_tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SXwIaDIxCaL"
      },
      "source": [
        "# Import and initialize dataset\n",
        "import plankton_images_tfds\n",
        "# load dataset the first time\n",
        "ds, info = tfds.load('plankton_images_tfds', as_supervised=True, with_info=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgtOOCgj9BRi"
      },
      "source": [
        "# Train prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSgn-RMUxCaM"
      },
      "source": [
        "# Image preprocess method\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "\n",
        "def preprocess(img, label):\n",
        "  image = tf.image.resize(img, [img_height, img_width]) / 255\n",
        "  return tf.image.rgb_to_grayscale(image), label\n",
        "\n",
        "# Ensure shuffle consistency\n",
        "seed = 42 #@param {type:\"integer\"}\n",
        "tf.random.set_seed(seed)\n",
        "read_config = tfds.ReadConfig(shuffle_seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgas-z4M9BRl"
      },
      "source": [
        "#@title Training hyperparams\n",
        "k = 10 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "epochs = 10 #@param {type:\"integer\"}\n",
        "# Ensure shuffle consistency\n",
        "seed = 7 #@param {type:\"integer\"}\n",
        "tf.random.set_seed(seed)\n",
        "read_config = tfds.ReadConfig(shuffle_seed=seed)\n",
        "\n",
        "# Configure dataset for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = tfds.load(name='plankton_images_tfds',\n",
        "                     split='train[0%:80%]',\n",
        "                     shuffle_files=True,\n",
        "                     as_supervised=True,\n",
        "                     read_config=read_config)\n",
        "train_ds = train_ds.map(preprocess).batch(32)\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = tfds.load(name='plankton_images_tfds',\n",
        "                   split='train[80%:90%]',\n",
        "                   shuffle_files=True,\n",
        "                   as_supervised=True,\n",
        "                   read_config=read_config)\n",
        "val_ds = val_ds.map(preprocess).batch(32)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDm8e8EtxCaL"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JlIWFXqbtWK"
      },
      "source": [
        "# Check backend\n",
        "!cat ~/.keras/keras.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMJKyJHP--qJ"
      },
      "source": [
        "# Hyperband optimization for model archtecture+\n",
        "class CNNHyperModel(kt.HyperModel):\n",
        "  def __init__(self, input_shape, num_classes):\n",
        "    self.input_shape = input_shape\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  def build(self, hp):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(InputLayer(input_shape=(img_height, img_width, 1)))\n",
        "\n",
        "    for i in range(hp.Int('num_blocks', min_value=1, max_value=10, step=1)):\n",
        "\n",
        "      model.add(Conv2D(\n",
        "          hp.Int('num_kernel', min_value=1, max_value=64, step=1),\n",
        "          (hp.Int('kernel_width', min_value=1, max_value=3, step=1),hp.Int('kernel_height', min_value=1, max_value=3, step=1)),\n",
        "          (hp.Int('conv_stride_width', min_value=1, max_value=3, step=1),hp.Int('conv_stride_height', min_value=1, max_value=3, step=1)),\n",
        "          padding='same',\n",
        "          activation='relu'))\n",
        "\n",
        "      model.add(MaxPooling2D(\n",
        "          (hp.Int('pool_width', min_value=1, max_value=3, step=1),hp.Int('pool_height', min_value=1, max_value=3, step=1)),\n",
        "          (hp.Int('pool_stride_width', min_value=1, max_value=3, step=1),hp.Int('pool_stride_height', min_value=1, max_value=3, step=1)),\n",
        "          padding='same'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=hp.Float(\n",
        "                'learning_rate',\n",
        "                min_value=1e-4,\n",
        "                max_value=1e-2,\n",
        "            )\n",
        "        ),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "hypermodel = CNNHyperModel(input_shape=(180, 180, 1), num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28Bp6sFs6_tt"
      },
      "source": [
        "tuner = kt.tuners.hyperband.Hyperband(\n",
        "    hypermodel,\n",
        "    objective='accuracy',\n",
        "    max_epochs=epochs,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW_OH6cg8FH3"
      },
      "source": [
        "tuner.search_space_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6_ML3j-8Xyk"
      },
      "source": [
        "tuner.search(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_ds,\n",
        "    #class_weight=class_weight,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbuA3J0-u8ry"
      },
      "source": [
        "model = tuner.get_best_models(num_models=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlPSMKW-NgeJ"
      },
      "source": [
        "# Saved tuned model\n",
        "%cd /content/gdrive/My Drive/Colab/plankton/model/\n",
        "model.save('tuned_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALB_DJc8xCaM"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dos765H1xCaN"
      },
      "source": [
        "tf.keras.utils.plot_model(model,\n",
        "                          to_file='model.png',\n",
        "                          show_shapes=True,\n",
        "                          show_dtype=True,\n",
        "                          show_layer_names=True,\n",
        "                          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2S-jcidxCaP"
      },
      "source": [
        "# Train and evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZXQjNdFn_rg"
      },
      "source": [
        "## Re-randomize model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5z92SeULKj6"
      },
      "source": [
        "# Get hypertuned model in case\n",
        "%cd /content/gdrive/My Drive/Colab/plankton/model/\n",
        "model = tf.keras.models.load_model('tuned_model')\n",
        "\n",
        "# Check architecture\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccAGxK3rWs9L"
      },
      "source": [
        "# Clone and compare weights\n",
        "original_weights = model.get_weights()\n",
        "print(\"Original weights\", original_weights[0])\n",
        "print(\"========================================================\")\n",
        "print(\"========================================================\")\n",
        "print(\"========================================================\")\n",
        "model_cloned = tf.keras.models.clone_model(model)\n",
        "new_weights = model_cloned.get_weights()\n",
        "print(\"New weights\", new_weights[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9crwqjPIk2JJ"
      },
      "source": [
        "print(model.optimizer.learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYCMFB8xepce"
      },
      "source": [
        "model_cloned.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=model.optimizer.learning_rate),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "# Learning rate for saved model\n",
        "print(model_cloned.optimizer.learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9UgBqjlnN2h"
      },
      "source": [
        "model = model_cloned\n",
        "model_weights = model.get_weights()\n",
        "print(model_weights[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjYE0yQBmZjb"
      },
      "source": [
        "# make sure weights are randomized\n",
        "test_ds = tfds.load(name='plankton_images_tfds',\n",
        "                    split='train[90%:100%]',\n",
        "                    shuffle_files=True,\n",
        "                    as_supervised=True,\n",
        "                    read_config=read_config)\n",
        "test_ds = test_ds.map(preprocess).batch(32)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_history = model.evaluate(\n",
        "    test_ds\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnNbGIeRoMaM"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNP494_huGQB"
      },
      "source": [
        "### loss convergence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdaevmKzPuTb"
      },
      "source": [
        "# Pickle in case of RAM outage\n",
        "import pickle\n",
        "\n",
        "#@title Training hyperparams\n",
        "k = 10 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "batch = 16 #@param {type:\"slider\", min:0, max:128, step:1}\n",
        "epochs =  100000#@param {type:\"integer\"}\n",
        "\n",
        "# Configure dataset for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Record accuracy histories for each fold\n",
        "acc_histories = []\n",
        "\n",
        "# Reset model weights\n",
        "reset_model = lambda model : model.set_weights(model_weights)\n",
        "reset_model(model)\n",
        "\n",
        "# Early stopping for convergence\n",
        "convergence_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=1e-20,\n",
        "    patience=3,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        ")\n",
        "\n",
        "dataset = 'plankton_images_tfds'\n",
        "crs_vld_begin = 0\n",
        "crs_vld_end = 90\n",
        "test_begin = 90\n",
        "test_end = 100\n",
        "\n",
        "\n",
        "def cross_validate(dataset, train_range, val_range):\n",
        "  train_ds = tfds.load(name=dataset,\n",
        "                       split=train_range,\n",
        "                       shuffle_files=True,\n",
        "                       as_supervised=True,\n",
        "                       read_config=read_config)\n",
        "  train_ds = train_ds.map(preprocess).batch(batch)\n",
        "  train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "  val_ds = tfds.load(name=dataset,\n",
        "                     split=val_range,\n",
        "                     shuffle_files=True,\n",
        "                     as_supervised=True,\n",
        "                     read_config=read_config)\n",
        "  val_ds = val_ds.map(preprocess).batch(batch)\n",
        "  val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "  acc_history = model.fit(\n",
        "      train_ds,\n",
        "      epochs=epochs,\n",
        "      callbacks=[convergence_callback],\n",
        "      validation_data=val_ds,\n",
        "  )\n",
        "  acc_histories.append(acc_history)\n",
        "  with open('acc_histories.pkl', 'wb') as f:\n",
        "    pickle.dump(acc_histories, f)\n",
        "  reset_model(model)\n",
        "\n",
        "\n",
        "for fold in range(k):\n",
        "  print(f\"FOLD {fold + 1}\", end=\": \")\n",
        "  \n",
        "  val_end = crs_vld_end - fold*(100/k-1)\n",
        "  val_start = val_end - 100/k + 1\n",
        "  \n",
        "  val_range = f'train[{val_start}%:{val_end}%]'\n",
        "  \n",
        "  # Validate ending\n",
        "  if val_end == crs_vld_end:\n",
        "    train_start = crs_vld_begin\n",
        "    train_end = val_start\n",
        "    train_range = f'train[{train_start}%:{train_end}%]'\n",
        "  # Validate beginning\n",
        "  elif val_start == crs_vld_begin:\n",
        "    train_start = val_end\n",
        "    train_end = crs_vld_end\n",
        "    train_range = f'train[{train_start}%:{train_end}%]'\n",
        "  # Middle\n",
        "  else:\n",
        "    train_start = crs_vld_begin\n",
        "    train_mid1 = val_start\n",
        "    train_mid2 = val_end\n",
        "    train_end = crs_vld_end\n",
        "    train_range = f'train[{train_start}%:{train_mid1}%]+train[{train_mid2}%:{train_end}%]'\n",
        "  \n",
        "  cross_validate(dataset, train_range, val_range)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLxr__8HlS8P"
      },
      "source": [
        "# Final training before testing\n",
        "print('Fold',10)\n",
        "train_ds = tfds.load(name='plankton_images_tfds',\n",
        "                     split='train[0%:90%]',\n",
        "                     shuffle_files=True,\n",
        "                     as_supervised=True,\n",
        "                     read_config=read_config)\n",
        "train_ds = train_ds.map(preprocess).batch(batch)\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = tfds.load(name='plankton_images_tfds',\n",
        "                   split='train[90%:100%]',\n",
        "                   shuffle_files=True,\n",
        "                   as_supervised=True,\n",
        "                   read_config=read_config)\n",
        "val_ds = val_ds.map(preprocess).batch(batch)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "acc_history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=[convergence_callback],\n",
        "    validation_data=val_ds,\n",
        "    #class_weight=class_weight,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCiZGKXLQp3J"
      },
      "source": [
        "test_ds = tfds.load(name='plankton_images_tfds',\n",
        "                    split='train[90%:100%]',\n",
        "                    shuffle_files=True,\n",
        "                    as_supervised=True,\n",
        "                    read_config=read_config)\n",
        "test_ds = test_ds.map(preprocess).batch(batch)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_history = model.evaluate(\n",
        "    test_ds\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('acc_histories.pkl', 'rb') as f:\n",
        "  acc = pickle.load(f)"
      ],
      "metadata": {
        "id": "vYqhEaq-auLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list all data in history\n",
        "for acc in acc_histories:\n",
        "  print(acc.history.keys())\n",
        "\n",
        "fold_num = 1\n",
        "for acc in acc_histories:\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(acc.history['accuracy'])\n",
        "  plt.plot(acc.history['val_accuracy'])\n",
        "  title = 'Fold ' + str(fold_num) + ': model accuracy'\n",
        "  plt.title(title)\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(acc.history['loss'])\n",
        "  plt.plot(acc.history['val_loss'])\n",
        "  title = 'Fold ' + str(fold_num) + ': model loss'\n",
        "  plt.title(title)\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  fold_num += 1"
      ],
      "metadata": {
        "id": "QrKDR27aa1mk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}